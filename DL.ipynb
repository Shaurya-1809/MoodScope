{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T12:19:45.702065300Z",
     "start_time": "2023-08-03T12:19:44.617326300Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\shaur\\\\OneDrive\\\\Desktop\\\\archive (2\\\\archive (2)\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 36\u001b[0m\n\u001b[0;32m     26\u001b[0m datagen \u001b[39m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m     27\u001b[0m     rescale\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m,       \u001b[39m# rescale pixel values from [0, 255] to [0, 1]\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     rotation_range\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,    \u001b[39m# randomly rotate images in the range (degrees, 0 to 180)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     horizontal_flip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,  \u001b[39m# randomly flip images\u001b[39;00m\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[39m# Generator for the training data\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m train_generator \u001b[39m=\u001b[39m datagen\u001b[39m.\u001b[39;49mflow_from_directory(\n\u001b[0;32m     37\u001b[0m     train_dir,\n\u001b[0;32m     38\u001b[0m     target_size\u001b[39m=\u001b[39;49mimage_size,\n\u001b[0;32m     39\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     40\u001b[0m     color_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgrayscale\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     41\u001b[0m     class_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcategorical\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[39m# Generator for the validation data\u001b[39;00m\n\u001b[0;32m     45\u001b[0m validation_generator \u001b[39m=\u001b[39m datagen\u001b[39m.\u001b[39mflow_from_directory(\n\u001b[0;32m     46\u001b[0m     validation_dir,\n\u001b[0;32m     47\u001b[0m     target_size\u001b[39m=\u001b[39mimage_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     class_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     51\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shaur\\anaconda3\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:1648\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[1;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m   1562\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflow_from_directory\u001b[39m(\n\u001b[0;32m   1563\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1564\u001b[0m     directory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1578\u001b[0m     keep_aspect_ratio\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1579\u001b[0m ):\n\u001b[0;32m   1580\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[0;32m   1581\u001b[0m \n\u001b[0;32m   1582\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1646\u001b[0m \u001b[39m            and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[0;32m   1647\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1648\u001b[0m     \u001b[39mreturn\u001b[39;00m DirectoryIterator(\n\u001b[0;32m   1649\u001b[0m         directory,\n\u001b[0;32m   1650\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1651\u001b[0m         target_size\u001b[39m=\u001b[39;49mtarget_size,\n\u001b[0;32m   1652\u001b[0m         color_mode\u001b[39m=\u001b[39;49mcolor_mode,\n\u001b[0;32m   1653\u001b[0m         keep_aspect_ratio\u001b[39m=\u001b[39;49mkeep_aspect_ratio,\n\u001b[0;32m   1654\u001b[0m         classes\u001b[39m=\u001b[39;49mclasses,\n\u001b[0;32m   1655\u001b[0m         class_mode\u001b[39m=\u001b[39;49mclass_mode,\n\u001b[0;32m   1656\u001b[0m         data_format\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_format,\n\u001b[0;32m   1657\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1658\u001b[0m         shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1659\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m   1660\u001b[0m         save_to_dir\u001b[39m=\u001b[39;49msave_to_dir,\n\u001b[0;32m   1661\u001b[0m         save_prefix\u001b[39m=\u001b[39;49msave_prefix,\n\u001b[0;32m   1662\u001b[0m         save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[0;32m   1663\u001b[0m         follow_links\u001b[39m=\u001b[39;49mfollow_links,\n\u001b[0;32m   1664\u001b[0m         subset\u001b[39m=\u001b[39;49msubset,\n\u001b[0;32m   1665\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation,\n\u001b[0;32m   1666\u001b[0m         dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype,\n\u001b[0;32m   1667\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\shaur\\anaconda3\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[1;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[0;32m    562\u001b[0m     classes \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 563\u001b[0m     \u001b[39mfor\u001b[39;00m subdir \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(directory)):\n\u001b[0;32m    564\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    565\u001b[0m             classes\u001b[39m.\u001b[39mappend(subdir)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\shaur\\\\OneDrive\\\\Desktop\\\\archive (2\\\\archive (2)\\\\train'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Directories for your train and validation datasets\n",
    "train_dir = (r'C:\\Users\\shaur\\OneDrive\\Desktop\\archive (2\\archive (2)\\train')\n",
    "validation_dir = (r'C:\\Users\\shaur\\OneDrive\\Desktop\\archive (2\\archive (2)\\test')\n",
    "\n",
    "# Dimensions to which to resize input images\n",
    "image_size = (48, 48)\n",
    "# image_size = (224, 224)# for ggogle net model\n",
    "# Batch size\n",
    "batch_size = 48\n",
    "\n",
    "# # Rescale pixel values from [0, 255] to [0, 1]\n",
    "# datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# # Generator for the training data\n",
    "# train_generator = datagen.flow_from_directory(\n",
    "#     train_dir,\n",
    "#     target_size=image_size,\n",
    "#     batch_size=batch_size,\n",
    "#     color_mode='grayscale',  # because the FER2013 images are grayscale\n",
    "#     class_mode='categorical'  # because we have multiple classes (emotions)\n",
    "# )\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,       # rescale pixel values from [0, 255] to [0, 1]\n",
    "    rotation_range=20,    # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.2,       # randomly zoom image \n",
    "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    ")\n",
    "\n",
    "# Generator for the training data\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Generator for the validation data\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd990617a961e8df",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mAdam(), loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     22\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(\n\u001b[1;32m---> 24\u001b[0m     train_generator,\n\u001b[0;32m     25\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_generator),\n\u001b[0;32m     26\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,  \u001b[39m# for example\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     validation_data\u001b[39m=\u001b[39mvalidation_generator,\n\u001b[0;32m     28\u001b[0m     validation_steps\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(validation_generator)\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39memotion_model.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))  # 7 is the number of classes (emotions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=10,  # for example\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator)\n",
    ")\n",
    "model.save('emotion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d062043c100f7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-03T11:58:26.676568800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "599/599 [==============================] - 733s 1s/step - loss: 1.8208 - accuracy: 0.2447 - val_loss: 1.7912 - val_accuracy: 0.2458\n",
      "Epoch 2/50\n",
      "311/599 [==============>...............] - ETA: 18s - loss: 1.7952 - accuracy: 0.2568"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout layer after first pooling layer\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Dropout(0.25))  # Dropout layer after second pooling layer\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout layer after third pooling layer\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout layer before final layer\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))  # 7 is the number of classes (emotions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=50,  # for example\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator)\n",
    ")\n",
    "\n",
    "model.save('emotion_model.h5')\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 7\n",
    "\n",
    "# Predict the classes on the validation dataset\n",
    "Y_pred = model.predict(validation_generator, len(validation_generator))\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Get the true class labels\n",
    "y_true = validation_generator.classes\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=range(num_classes))\n",
    "cmd.plot()\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e0e8b48f6b48e3e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-03T12:20:07.136426400Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m checkpoint_filepath \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodel.\u001b[39m\u001b[39m{epoch:02d}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{val_loss:.2f}\u001b[39;00m\u001b[39m.h5\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     67\u001b[0m model_checkpoint_callback \u001b[39m=\u001b[39m ModelCheckpoint(filepath\u001b[39m=\u001b[39mcheckpoint_filepath, save_best_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 69\u001b[0m accuracy_comparison_callback \u001b[39m=\u001b[39m AccuracyComparisonCallback(validation_generator)\n\u001b[0;32m     71\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m     72\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(\n\u001b[0;32m     73\u001b[0m     train_generator,\n\u001b[0;32m     74\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_generator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     callbacks\u001b[39m=\u001b[39m[model_checkpoint_callback, accuracy_comparison_callback]\n\u001b[0;32m     79\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'validation_generator' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Custom callback for accuracy comparison after each epoch\n",
    "class AccuracyComparisonCallback(Callback):\n",
    "    def __init__(self, validation_generator):\n",
    "        super(AccuracyComparisonCallback, self).__init__()\n",
    "        self.validation_generator = validation_generator\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        Y_pred = self.model.predict(self.validation_generator, len(self.validation_generator))\n",
    "        y_pred = np.argmax(Y_pred, axis=1)\n",
    "        y_true = self.validation_generator.classes\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        print(f'Validation accuracy: {logs[\"val_accuracy\"]}, Predicted accuracy: {accuracy}')\n",
    "\n",
    "# # Define the model\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Dropout(0.25))  # Dropout layer after first pooling layer\n",
    "# \n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Dropout(0.25))  # Dropout layer after second pooling layer\n",
    "# \n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5))  # Dropout layer before final layer\n",
    "# \n",
    "# model.add(Dense(7, activation='softmax'))  # 7 is the number of classes (emotions)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "model.add(BatchNormalization())  # Batch normalization layer\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout layer after first pooling layer\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())  # Batch normalization layer\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout layer after second pooling layer\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())  # Batch normalization layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout layer before final layer\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))  # 7 is the number of classes (emotions)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Specify the checkpoint file and define the ModelCheckpoint callback\n",
    "checkpoint_filepath = 'model.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=False)\n",
    "\n",
    "accuracy_comparison_callback = AccuracyComparisonCallback(validation_generator)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=50,  # for example\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[model_checkpoint_callback, accuracy_comparison_callback]\n",
    ")\n",
    "\n",
    "model.save('emotion_model.h5')\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 7\n",
    "\n",
    "# Predict the classes on the validation dataset\n",
    "Y_pred = model.predict(validation_generator, len(validation_generator))\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Get the true class labels\n",
    "y_true = validation_generator.classes\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=range(num_classes))\n",
    "cmd.plot()\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bafc538d9d7c91",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load pre-trained emotion classification model\n",
    "emotion_model = load_model('emotion_model.h5')\n",
    "\n",
    "# Emotion labels\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Load the image\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"Press 'q' or 'Q' to take a picture\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key & 0xFF == ord('q') or key & 0xFF == ord('Q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "# Convert to grayscale for face detection\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Perform face detection\n",
    "faces = detector.detect_faces(frame)\n",
    "\n",
    "# Loop over all detected faces\n",
    "for person in faces:\n",
    "    bounding_box = person['box']\n",
    "\n",
    "    x, y, w, h = bounding_box\n",
    "\n",
    "    # Extract face\n",
    "    face = gray[y:y+h, x:x+w]\n",
    "\n",
    "    # Resize and normalize the face image to the size your emotion model expects\n",
    "    face = cv2.resize(face, (48, 48))\n",
    "    face = face / 255.0\n",
    "\n",
    "    # The model expects a 4D tensor, so expand dimensions\n",
    "    face = np.expand_dims(face, axis=(0, -1))\n",
    "\n",
    "    # Predict emotion\n",
    "    emotion = emotion_model.predict(face)\n",
    "\n",
    "    # Get the emotion label\n",
    "    emotion_label = emotion_labels[np.argmax(emotion)]\n",
    "\n",
    "    # Print the emotion label\n",
    "    print(emotion_label)\n",
    "\n",
    "    # Draw the bounding box of the face along with the emotion label\n",
    "    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    cv2.putText(frame, emotion_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# Show the image\n",
    "cv2.imshow('Image', frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
